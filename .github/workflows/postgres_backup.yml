 



 name: Crawl and Backup GitHub Repos

on:
  workflow_dispatch:       # Manual trigger
  schedule:
    - cron: '0 2 * * *'    # Every day at 02:00 UTC

jobs:
  crawl:
    runs-on: ubuntu-latest
    # ------------------------------------------------------------------
    # One Postgres container – we will create the extra DBs inside it
    # ------------------------------------------------------------------
    services:
      postgres:
        image: postgres:17
        env:
          POSTGRES_USER: ${{ secrets.DB_USER }}
          POSTGRES_PASSWORD: ${{ secrets.DB_PASSWORD }}
          POSTGRES_DB: ${{ secrets.DB_NAME }}   # main DB
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U ${{ secrets.DB_USER }}"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

    steps:
      # ---------------------------------------------------------
      # Checkout code
      # ---------------------------------------------------------
      - name: Checkout repository
        uses: actions/checkout@v3

      # ---------------------------------------------------------
      # Python environment
      # ---------------------------------------------------------
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r crawler/requirements.txt

      # ---------------------------------------------------------
      # Create shard databases + apply schema
      # ---------------------------------------------------------
      - name: Create shard databases & apply schema
        env:
          PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          # Create the two shard DBs
          createdb -h localhost -U ${{ secrets.DB_USER }} ${{ secrets.DB_NAME_SHARD1 }}
          createdb -h localhost -U ${{ secrets.DB_USER }} ${{ secrets.DB_NAME_SHARD2 }}

          # Apply the same schema to every DB
          psql -h localhost -U ${{ secrets.DB_USER }} -d ${{ secrets.DB_NAME }}          -f crawler/setup_postgres.sql
          psql -h localhost -U ${{ secrets.DB_USER }} -d ${{ secrets.DB_NAME_SHARD1 }} -f crawler/setup_postgres.sql
          psql -h localhost -U ${{ secrets.DB_USER }} -d ${{ secrets.DB_NAME_SHARD2 }} -f crawler/setup_postgres.sql

      # ---------------------------------------------------------
      # Run the crawler (it will write to all three DBs)
      # ---------------------------------------------------------
      - name: Crawl GitHub repos
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GH_TOKENS: ${{ secrets.GITHUB_TOKENS }}   # optional, for token rotation
          DATABASE_URL: postgresql://${{ secrets.DB_USER }}:${{ secrets.DB_PASSWORD }}@localhost:5432/${{ secrets.DB_NAME }}
          DATABASE_URL_1: postgresql://${{ secrets.DB_USER }}:${{ secrets.DB_PASSWORD }}@localhost:5432/${{ secrets.DB_NAME_SHARD1 }}
          DATABASE_URL_2: postgresql://${{ secrets.DB_USER }}:${{ secrets.DB_PASSWORD }}@localhost:5432/${{ secrets.DB_NAME_SHARD2 }}
        run: |
          cd crawler
          python crawl_stars.py   # <-- your script, now shard-aware

  # ----------------------------------------------------------------------
  # Backup job – runs only after a successful crawl
  # ----------------------------------------------------------------------
  backup:
    runs-on: ubuntu-latest
    needs: crawl
    services:
      postgres:
        image: postgres:17
        env:
          POSTGRES_USER: ${{ secrets.DB_USER }}
          POSTGRES_PASSWORD: ${{ secrets.DB_PASSWORD }}
          POSTGRES_DB: ${{ secrets.DB_NAME }}
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U ${{ secrets.DB_USER }}"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Create backup folder
        run: mkdir -p backups

      # ---------------------------------------------------------
      # Dump **all** databases (main + shards)
      # ---------------------------------------------------------
      - name: Dump all databases
        env:
          PGPASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          DATE=$(date +"%Y%m%d_%H%M%S")

          # Helper function
          dump_db() {
            local db_name=$1
            local out_file="backups/${db_name}_backup_${DATE}.sql"
            echo "Dumping $db_name → $out_file"
            pg_dump -U ${{ secrets.DB_USER }} -h localhost -F c -b -v -f "$out_file" "$db_name"
          }

          dump_db ${{ secrets.DB_NAME }}
          dump_db ${{ secrets.DB_NAME_SHARD1 }}
          dump_db ${{ secrets.DB_NAME_SHARD2 }}

      # ---------------------------------------------------------
      # Commit & push the backups
      # ---------------------------------------------------------
      - name: Commit and Push Backup
        run: |
          git config --local user.name "github-actions"
          git config --local user.email "actions@github.com"
          git add backups/
          git commit -m "DB Backup (main + shards) - $(date +"%Y-%m-%d %H:%M:%S")" || echo "No changes to commit"
          git push
